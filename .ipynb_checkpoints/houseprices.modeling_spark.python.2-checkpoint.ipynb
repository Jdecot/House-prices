{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import statsmodels.api as sm\n",
    "\n",
    "import copy\n",
    "from statsmodels.formula.api import ols\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import LibrairiePerso_v4_8 as ownLibrary\n",
    "\n",
    "'''\n",
    "import sys\n",
    "sys.path.append(\"c:\\python38\\lib\\site-packages\")\n",
    "'''\n",
    "#import importlib\n",
    "#importlib.reload(ownLibrary)\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor df in [X_train, X_test]:\\n    df.drop(['SalePrice'], axis=1, inplace=True)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "\n",
    "train_rwrk = pd.read_csv(cwd + \"\\\\data\\\\train_step_2.csv\", sep=\",\")\n",
    "#y_train=X_train['SalePrice']\n",
    "\n",
    "test_rwrk = pd.read_csv(cwd + \"\\\\data\\\\test_step_2.csv\", sep=\",\")\n",
    "#y_test=X_test['SalePrice']\n",
    "\n",
    "submission = pd.read_csv(cwd + \"\\\\data\\\\submission_step_2.csv\", sep=\",\")\n",
    "\n",
    "'''\n",
    "for df in [X_train, X_test]:\n",
    "    df.drop(['SalePrice'], axis=1, inplace=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with SparkML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "except ImportError as e:\n",
    "    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "spark_train = spark.createDataFrame(train_rwrk)\n",
    "spark_test = spark.createDataFrame(test_rwrk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+---------+------------------+---------+\n",
      "| Id|BsmtUnfSF|hasgarage|YearBuilt|     SalePrice_log|SalePrice|\n",
      "+---+---------+---------+---------+------------------+---------+\n",
      "|211|      396|        0|        1| 11.49272275765271|    98000|\n",
      "|319|      360|        1|        3|12.468436909997664|   260000|\n",
      "+---+---------+---------+---------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_train.createOrReplaceTempView('spark_train')\n",
    "spark.sql(\"SELECT Id, BsmtUnfSF, hasgarage, YearBuilt, SalePrice_log, SalePrice FROM spark_train limit 2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert feature to Int and define their type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for feature in spark_train.columns:\n",
    "    spark_train = spark_train.withColumn(feature, spark_train[feature].cast(IntegerType()))\n",
    "    spark_test = spark_test.withColumn(feature, spark_test[feature].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_features = ['Id', 'SalePrice_log', 'SalePrice']\n",
    "quantitative_features = ['MasVnrArea', 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'GarageArea', 'ScreenPorch', 'MiscVal', 'TotalSF', 'Total_sqr_footage', 'Total_Bathrooms', 'Total_porch_sf', '1stFlrSF²', '1stFlrSFGarageArea', 'GarageArea²', 'PR3_6', 'PR3_7', 'PR3_8', 'PR3_9', 'PR3_10', 'PR3_11', 'PR3_12', 'PR3_13', 'PR3_14', 'PR3_15', 'PR3_16', 'PR3_17', 'PR3_18', 'PR3_19', 'PR3_20', 'PR3_21', 'PR3_22', 'PR3_23', 'PR3_24', 'PR3_25', 'PR3_26', 'PR3_27', 'PR3_28', 'PR3_29', 'PR3_30', 'PR3_31', 'PR3_32', 'PR3_33', 'PR3_34', 'PR3_35', 'PR3_36', 'PR3_37', 'PR3_38', 'PR3_39', 'PR3_40', 'PR3_41', 'PR3_42', 'PR3_43', 'PR3_44', 'PR3_45', 'PR3_46', 'PR3_47', 'PR3_48', 'PR3_49', 'PR3_50', 'PR3_51', 'PR3_52', 'PR3_53', 'PR3_54', 'PR3_55']\n",
    "boolean_features = [ 'haspool', 'has2ndfloor', 'hasgarage', 'hasbsmt', 'hasfireplace']\n",
    "categorical_features = ['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Alley', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtFinSF2', 'HeatingQC', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'Functional', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageQual', 'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', 'Fence', 'MoSold', 'YrSold', 'YrBltAndRemod', 'NewFirePlaces', 'NewExterQualCond', 'NewCentrAirElec', 'NewKitchen', 'NewSale']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_features = [var+'_indexed' for var in categorical_features ]\n",
    "encoded_features = [var+'_ohe' for var in categorical_features ]\n",
    "\n",
    "indexer = StringIndexer()\n",
    "indexer.setInputCols(categorical_features)\n",
    "indexer.setOutputCols(indexed_features)\n",
    "indexed_train = indexer.fit(spark_train).transform(spark_train)\n",
    "indexed_test = indexer.fit(spark_test).transform(spark_test)\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "encoder.setInputCols(indexed_features)\n",
    "encoder.setOutputCols(encoded_features)\n",
    "encoded_train = encoder.fit(indexed_train).transform(indexed_train)\n",
    "encoded_test = encoder.fit(indexed_test).transform(indexed_test)\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=quantitative_features+boolean_features+encoded_features, outputCol=\"quant_features\")\n",
    "vectorized_train = vectorAssembler.transform(encoded_train)\n",
    "vectorized_test = vectorAssembler.transform(encoded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [indexer, encoder, vectorAssembler])\n",
    "\n",
    "df_train = pipeline.fit(spark_train).transform(spark_train)\n",
    "df_test = pipeline.fit(spark_test).transform(spark_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-------------+---------+\n",
      "|  Id|      quant_features|SalePrice_log|SalePrice|\n",
      "+----+--------------------+-------------+---------+\n",
      "| 211|(196,[1,2,3,4,6,1...|           11|    98000|\n",
      "| 319|(196,[0,1,2,3,4,5...|           12|   260000|\n",
      "| 240|(196,[1,2,3,4,5,6...|           11|   113000|\n",
      "| 987|(196,[2,3,4,5,6,7...|           11|   117000|\n",
      "|1417|(196,[2,3,4,5,6,7...|           11|   122500|\n",
      "+----+--------------------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.createOrReplaceTempView('df_train')\n",
    "#spark_train.show()\n",
    "spark.sql(\"SELECT Id, quant_features, SalePrice_log, SalePrice FROM df_train limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model with Spark using all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8985154906421481"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression(featuresCol = 'quant_features', labelCol='SalePrice', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "supermodel = lr.fit(df_train)\n",
    "supermodel.summary.r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|  Id|        prediction|\n",
      "+----+------------------+\n",
      "| 211|   96560.428756378|\n",
      "| 319| 335061.5902159068|\n",
      "| 240|115373.79903877417|\n",
      "| 987|114709.72185573752|\n",
      "|1417|158447.88400895565|\n",
      "+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = supermodel.transform(df_train)\n",
    "predictions.select(\"Id\",\"prediction\").show(5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1762.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 18.0 failed 1 times, most recent failure: Lost task 1.0 in stage 18.0 (TID 122) (DESKTOP-2KQMR2K executor driver): org.apache.spark.SparkException: Failed to execute user defined function(PredictionModel$$Lambda$4440/137865441: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 195, y.size = 196\r\n\tat scala.Predef$.require(Predef.scala:281)\r\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:115)\r\n\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:736)\r\n\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:686)\r\n\tat org.apache.spark.ml.PredictionModel.$anonfun$transformImpl$1(Predictor.scala:251)\r\n\tat org.apache.spark.ml.PredictionModel.$anonfun$transformImpl$1$adapted(Predictor.scala:250)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(PredictionModel$$Lambda$4440/137865441: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 195, y.size = 196\r\n\tat scala.Predef$.require(Predef.scala:281)\r\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:115)\r\n\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:736)\r\n\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:686)\r\n\tat org.apache.spark.ml.PredictionModel.$anonfun$transformImpl$1(Predictor.scala:251)\r\n\tat org.apache.spark.ml.PredictionModel.$anonfun$transformImpl$1$adapted(Predictor.scala:250)\r\n\t... 17 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-ba6c4c319cd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msupermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresult\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'prediction'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\python39\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m         \"\"\"\n\u001b[0;32m    676\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python39\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python39\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python39\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1762.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 18.0 failed 1 times, most recent failure: Lost task 1.0 in stage 18.0 (TID 122) (DESKTOP-2KQMR2K executor driver): org.apache.spark.SparkException: Failed to execute user defined function(PredictionModel$$Lambda$4440/137865441: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 195, y.size = 196\r\n\tat scala.Predef$.require(Predef.scala:281)\r\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:115)\r\n\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:736)\r\n\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:686)\r\n\tat org.apache.spark.ml.PredictionModel.$anonfun$transformImpl$1(Predictor.scala:251)\r\n\tat org.apache.spark.ml.PredictionModel.$anonfun$transformImpl$1$adapted(Predictor.scala:250)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(PredictionModel$$Lambda$4440/137865441: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 195, y.size = 196\r\n\tat scala.Predef$.require(Predef.scala:281)\r\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:115)\r\n\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:736)\r\n\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:686)\r\n\tat org.apache.spark.ml.PredictionModel.$anonfun$transformImpl$1(Predictor.scala:251)\r\n\tat org.apache.spark.ml.PredictionModel.$anonfun$transformImpl$1$adapted(Predictor.scala:250)\r\n\t... 17 more\r\n"
     ]
    }
   ],
   "source": [
    "predictions = supermodel.transform(df_test)\n",
    "result= predictions.select('prediction').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [indexer, encoder])\n",
    "\n",
    "df_train_rfe = pipeline.fit(spark_train).transform(spark_train)\n",
    "df_test_rfe = pipeline.fit(spark_test).transform(spark_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "usable_features = quantitative_features+boolean_features+encoded_features\n",
    "def model_with_spark(train, usable_features):\n",
    "    vectorAssembler_rfe = VectorAssembler(inputCols=usable_features, outputCol=\"quant_features\")\n",
    "    vectorized_train = vectorAssembler_rfe.transform(train)\n",
    "\n",
    "    lr = LinearRegression(featuresCol = 'quant_features', labelCol='SalePrice', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "    model = lr.fit(vectorized_train)\n",
    "    predictions = model.transform(vectorized_train)\n",
    "    #predictions.select(\"prediction\").show(5);\n",
    "    result_pdf = predictions.select(\"prediction\").toPandas()\n",
    "    return result_pdf\n",
    "\n",
    "spark_df = model_with_spark(df_train_rfe, df_train_rfe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110121.601213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>304717.837082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>121359.107042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>119739.209683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>149559.354121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>189840.434462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>336276.683660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>304539.700956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>119860.003461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>261748.846514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1022 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         prediction\n",
       "0     110121.601213\n",
       "1     304717.837082\n",
       "2     121359.107042\n",
       "3     119739.209683\n",
       "4     149559.354121\n",
       "...             ...\n",
       "1017  189840.434462\n",
       "1018  336276.683660\n",
       "1019  304539.700956\n",
       "1020  119860.003461\n",
       "1021  261748.846514\n",
       "\n",
       "[1022 rows x 1 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_feature_elimination_rf(X_train, X_train_features_to_use, tol=0.0001):\n",
    "\n",
    "    features_to_remove = []\n",
    "    count = 1\n",
    "    # initial model using all the features\n",
    "    y_pred_test = model_with_spark(X_train, X_train_features_to_use)\n",
    "    y_test = X_train.select(\"SalePrice\").toPandas()\n",
    "    auc_score_all = ownLibrary.rsquared(y_test['SalePrice'], y_pred_test['prediction'])\n",
    "    print('r2 for all features : ' + str(round(auc_score_all,4)))\n",
    "    \n",
    "    for feature in X_train_features_to_use:\n",
    "        count += 1\n",
    "        \n",
    "        features_to_use = [x for x in X_train_features_to_use if x not in features_to_remove + [feature]]\n",
    "        y_pred_test = model_with_spark(X_train, features_to_use)\n",
    "        auc_score_int = ownLibrary.rsquared(y_test['SalePrice'], y_pred_test['prediction'])\n",
    "\n",
    "        diff_auc = auc_score_all - auc_score_int\n",
    "    \n",
    "        if diff_auc >= tol:\n",
    "\n",
    "            score = diff_auc\n",
    "            \n",
    "        else:\n",
    "\n",
    "            auc_score_all = auc_score_int\n",
    "            score = auc_score_int\n",
    "            \n",
    "            features_to_remove.append(feature)\n",
    "        #if count==6:\n",
    "        #    break\n",
    "        print('r2 : ' + str(round(auc_score_int,4)) + ' features to remove count : ' + str(len(features_to_remove)))\n",
    "    print('total features to remove: ', len(features_to_remove))  \n",
    "    features_to_keep = [x for x in X_train_features_to_use if x not in features_to_remove]\n",
    "    print('total features to keep: ', len(features_to_keep))\n",
    "    \n",
    "    return features_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 for all features : 0.8987\n",
      "r2 : 0.8982 features to remove count : 0\n",
      "r2 : 0.8987 features to remove count : 1\n",
      "r2 : 0.8988 features to remove count : 2\n",
      "r2 : 0.899 features to remove count : 3\n",
      "r2 : 0.8973 features to remove count : 3\n",
      "r2 : 0.8984 features to remove count : 3\n",
      "r2 : 0.8998 features to remove count : 4\n",
      "r2 : 0.8989 features to remove count : 4\n",
      "r2 : 0.8995 features to remove count : 4\n",
      "r2 : 0.8998 features to remove count : 5\n",
      "r2 : 0.8995 features to remove count : 5\n",
      "r2 : 0.9012 features to remove count : 6\n",
      "r2 : 0.9018 features to remove count : 7\n",
      "r2 : 0.8989 features to remove count : 7\n",
      "r2 : 0.8989 features to remove count : 7\n",
      "r2 : 0.9006 features to remove count : 7\n",
      "r2 : 0.8998 features to remove count : 7\n",
      "r2 : 0.8956 features to remove count : 7\n",
      "r2 : 0.8984 features to remove count : 7\n",
      "r2 : 0.8978 features to remove count : 7\n",
      "r2 : 0.9003 features to remove count : 7\n",
      "r2 : 0.8981 features to remove count : 7\n",
      "r2 : 0.8997 features to remove count : 7\n",
      "r2 : 0.8994 features to remove count : 7\n",
      "r2 : 0.9009 features to remove count : 7\n",
      "r2 : 0.9001 features to remove count : 7\n",
      "r2 : 0.8996 features to remove count : 7\n",
      "r2 : 0.9021 features to remove count : 8\n",
      "r2 : 0.8991 features to remove count : 8\n",
      "r2 : 0.8949 features to remove count : 8\n",
      "r2 : 0.9027 features to remove count : 9\n",
      "r2 : 0.9018 features to remove count : 9\n",
      "r2 : 0.8963 features to remove count : 9\n",
      "r2 : 0.9005 features to remove count : 9\n",
      "r2 : 0.9 features to remove count : 9\n",
      "r2 : 0.902 features to remove count : 9\n",
      "r2 : 0.9002 features to remove count : 9\n",
      "r2 : 0.8997 features to remove count : 9\n",
      "r2 : 0.9001 features to remove count : 9\n",
      "r2 : 0.8998 features to remove count : 9\n",
      "r2 : 0.9005 features to remove count : 9\n",
      "r2 : 0.902 features to remove count : 9\n",
      "r2 : 0.9021 features to remove count : 9\n",
      "r2 : 0.8997 features to remove count : 9\n",
      "r2 : 0.902 features to remove count : 9\n",
      "r2 : 0.9024 features to remove count : 9\n",
      "r2 : 0.901 features to remove count : 9\n",
      "r2 : 0.9038 features to remove count : 10\n",
      "r2 : 0.8992 features to remove count : 10\n",
      "r2 : 0.9033 features to remove count : 10\n",
      "r2 : 0.8996 features to remove count : 10\n",
      "r2 : 0.8986 features to remove count : 10\n",
      "r2 : 0.9017 features to remove count : 10\n",
      "r2 : 0.8987 features to remove count : 10\n",
      "r2 : 0.9038 features to remove count : 11\n",
      "r2 : 0.899 features to remove count : 11\n",
      "r2 : 0.9026 features to remove count : 11\n",
      "r2 : 0.9032 features to remove count : 11\n",
      "r2 : 0.9037 features to remove count : 12\n",
      "r2 : 0.9018 features to remove count : 12\n",
      "r2 : 0.8995 features to remove count : 12\n",
      "r2 : 0.9043 features to remove count : 13\n",
      "r2 : 0.9014 features to remove count : 13\n",
      "r2 : 0.8998 features to remove count : 13\n",
      "r2 : 0.9005 features to remove count : 13\n",
      "r2 : 0.8997 features to remove count : 13\n",
      "r2 : 0.9003 features to remove count : 13\n",
      "r2 : 0.9029 features to remove count : 13\n",
      "r2 : 0.8991 features to remove count : 13\n",
      "r2 : 0.9042 features to remove count : 14\n",
      "r2 : 0.904 features to remove count : 14\n",
      "r2 : 0.9009 features to remove count : 14\n",
      "r2 : 0.8987 features to remove count : 14\n",
      "r2 : 0.8995 features to remove count : 14\n",
      "r2 : 0.8988 features to remove count : 14\n",
      "r2 : 0.8998 features to remove count : 14\n",
      "r2 : 0.9043 features to remove count : 15\n",
      "r2 : 0.9026 features to remove count : 15\n",
      "r2 : 0.8997 features to remove count : 15\n",
      "r2 : 0.8992 features to remove count : 15\n",
      "r2 : 0.9019 features to remove count : 15\n",
      "r2 : 0.8953 features to remove count : 15\n",
      "r2 : 0.902 features to remove count : 15\n",
      "r2 : 0.9014 features to remove count : 15\n",
      "r2 : 0.9022 features to remove count : 15\n",
      "r2 : 0.8943 features to remove count : 15\n",
      "r2 : 0.896 features to remove count : 15\n",
      "r2 : 0.9016 features to remove count : 15\n",
      "r2 : 0.9026 features to remove count : 15\n",
      "r2 : 0.9006 features to remove count : 15\n",
      "r2 : 0.8997 features to remove count : 15\n",
      "r2 : 0.8994 features to remove count : 15\n",
      "r2 : 0.8992 features to remove count : 15\n",
      "r2 : 0.9 features to remove count : 15\n",
      "r2 : 0.8954 features to remove count : 15\n",
      "r2 : 0.8996 features to remove count : 15\n",
      "r2 : 0.8934 features to remove count : 15\n",
      "r2 : 0.8992 features to remove count : 15\n",
      "r2 : 0.9011 features to remove count : 15\n",
      "r2 : 0.9009 features to remove count : 15\n",
      "r2 : 0.8997 features to remove count : 15\n",
      "r2 : 0.9041 features to remove count : 15\n",
      "r2 : 0.9 features to remove count : 15\n",
      "r2 : 0.9004 features to remove count : 15\n",
      "r2 : 0.9006 features to remove count : 15\n",
      "r2 : 0.8988 features to remove count : 15\n",
      "r2 : 0.8984 features to remove count : 15\n",
      "r2 : 0.8997 features to remove count : 15\n",
      "r2 : 0.9006 features to remove count : 15\n",
      "r2 : 0.9022 features to remove count : 15\n",
      "r2 : 0.9015 features to remove count : 15\n",
      "r2 : 0.8968 features to remove count : 15\n",
      "r2 : 0.8992 features to remove count : 15\n",
      "r2 : 0.9013 features to remove count : 15\n",
      "r2 : 0.9016 features to remove count : 15\n",
      "r2 : 0.8992 features to remove count : 15\n",
      "r2 : 0.901 features to remove count : 15\n",
      "r2 : 0.9002 features to remove count : 15\n",
      "r2 : 0.9038 features to remove count : 15\n",
      "r2 : 0.9005 features to remove count : 15\n",
      "r2 : 0.8999 features to remove count : 15\n",
      "r2 : 0.9014 features to remove count : 15\n",
      "r2 : 0.901 features to remove count : 15\n",
      "r2 : 0.9008 features to remove count : 15\n",
      "r2 : 0.8994 features to remove count : 15\n",
      "r2 : 0.8935 features to remove count : 15\n",
      "r2 : 0.8999 features to remove count : 15\n",
      "total features to remove:  15\n",
      "total features to keep:  112\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-23c5ab0a32f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeatures_to_keep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecursive_feature_elimination_rf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train_rfe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musable_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "features_to_keep, score = recursive_feature_elimination_rf(df_train_rfe, usable_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9043316452720312"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train = model_with_spark(df_train_rfe, features_to_keep)\n",
    "y_train = df_train_rfe.select(\"SalePrice\").toPandas()\n",
    "rsquared = ownLibrary.rsquared(y_train['SalePrice'], y_pred_train['prediction'])\n",
    "rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9037777937689194"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = model_with_spark(df_test_rfe, features_to_keep)\n",
    "y_test = df_test_rfe.select(\"SalePrice\").toPandas()\n",
    "rsquared = ownLibrary.rsquared(y_test['SalePrice'], y_pred_test['prediction'])\n",
    "rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_feature_addition(dataset, dataset_features_to_use, model):\n",
    "    colonnes = dataset_features_to_use\n",
    "    features_to_keep = [colonnes[0]]\n",
    "\n",
    "    # set this value according to you.\n",
    "    threshold = 0.0001\n",
    "\n",
    "    # create your prefered model and  fit it to the training data.\n",
    "    model_one_feature = model\n",
    "    model_one_feature.fit(X_train[features_to_keep], y_train)\n",
    "\n",
    "    # evaluate against your metric.\n",
    "    y_pred_test = model_one_feature.predict(X_test[features_to_keep])\n",
    "    score =  rsquared(y_test, y_pred_test)\n",
    "\n",
    "    # start iterating from the feature.\n",
    "    for feature in colonnes[1:]:    \n",
    "        # fit model with  the selected features and the feature to be evaluated\n",
    "        #model = LinearRegression()\n",
    "        model.fit(X_train[features_to_keep + [feature]], y_train)\n",
    "        y_pred_test = model.predict(X_test[features_to_keep + [feature]])\n",
    "        score_int =  rsquared(y_test, y_pred_test)\n",
    "\n",
    "        # determine the drop in the roc-auc\n",
    "        diff_score = score_int - score\n",
    "\n",
    "        # compare the drop in roc-auc with the threshold\n",
    "        if diff_score >= threshold:\n",
    "            \n",
    "            # if the increase in the roc is bigger than the threshold\n",
    "            # we keep the feature and re-adjust the roc-auc to the new value\n",
    "            # considering the added feature\n",
    "            score = score_int\n",
    "            features_to_keep.append(feature)\n",
    "\n",
    "    # print the feature to keep.\n",
    "    #print(features_to_keep)\n",
    "    print(score)\n",
    "    print(len(features_to_keep))\n",
    "    return features_to_keep, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_feature_elimination_rf(X_train, X_train_features_to_use, tol=0.0001):\n",
    "\n",
    "    features_to_remove = []\n",
    "    count = 1\n",
    "    # initial model using all the features\n",
    "    y_pred_test = model_with_spark(X_train, X_train_features_to_use)\n",
    "    y_test = X_train.select(\"SalePrice\").toPandas()\n",
    "    auc_score_all = ownLibrary.rsquared(y_test['SalePrice'], y_pred_test['prediction'])\n",
    "    print('r2 for all features : ' + str(round(auc_score_all,4)))\n",
    "    \n",
    "    for feature in X_train_features_to_use:\n",
    "        count += 1\n",
    "        \n",
    "        features_to_use = [x for x in X_train_features_to_use if x not in features_to_remove + [feature]]\n",
    "        y_pred_test = model_with_spark(X_train, features_to_use)\n",
    "        auc_score_int = ownLibrary.rsquared(y_test['SalePrice'], y_pred_test['prediction'])\n",
    "\n",
    "        diff_auc = auc_score_all - auc_score_int\n",
    "    \n",
    "        if diff_auc >= tol:\n",
    "\n",
    "            score = diff_auc\n",
    "            \n",
    "        else:\n",
    "\n",
    "            auc_score_all = auc_score_int\n",
    "            score = auc_score_int\n",
    "            \n",
    "            features_to_remove.append(feature)\n",
    "        #if count==6:\n",
    "        #    break\n",
    "        print('r2 : ' + str(round(auc_score_int,4)) + ' features to remove count : ' + str(len(features_to_remove)))\n",
    "    print('total features to remove: ', len(features_to_remove))  \n",
    "    features_to_keep = [x for x in X_train_features_to_use if x not in features_to_remove]\n",
    "    print('total features to keep: ', len(features_to_keep))\n",
    "    \n",
    "    return features_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValidatorModel_2934efed8682\n"
     ]
    }
   ],
   "source": [
    "#https://gist.github.com/colbyford/184097b0ec37b2b35667dab2da57d349\n",
    "\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'quant_features', labelCol='SalePrice')\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "\n",
    "lrparamGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.05, 0.1, 0.3, 0.5])\n",
    "             #  .addGrid(lr.regParam, [0.01, 0.1, 0.5])\n",
    "             .addGrid(lr.elasticNetParam, [0.7, 0.8, 0.9, 1.0])\n",
    "             #  .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .addGrid(lr.maxIter, [5, 10, 20])\n",
    "             #  .addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "lrevaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"SalePrice\", metricName=\"rmse\")\n",
    "                                  \n",
    "# Create 5-fold CrossValidator\n",
    "lrcv = CrossValidator(estimator = lr,\n",
    "                    estimatorParamMaps = lrparamGrid,\n",
    "                    evaluator = lrevaluator,\n",
    "                    numFolds = 5)\n",
    "\n",
    "# Run cross validations\n",
    "lrcvModel = lrcv.fit(df_train)\n",
    "print(lrcvModel)\n",
    "                                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28262.27894925193"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrcvModel.bestModel.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8721715816934545"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrcvModel.bestModel.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Param (regParam):  0.05\n",
      "Best Param (MaxIter):  10\n",
      "Best Param (elasticNetParam):  0.7\n"
     ]
    }
   ],
   "source": [
    "print ('Best Param (regParam): ', lrcvModel.bestModel._java_obj.getRegParam())\n",
    "print ('Best Param (MaxIter): ', lrcvModel.bestModel._java_obj.getMaxIter())\n",
    "print ('Best Param (elasticNetParam): ', lrcvModel.bestModel._java_obj.getElasticNetParam())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 27488.32960360495\n"
     ]
    }
   ],
   "source": [
    "# Get Model Summary Statistics\n",
    "lrcvSummary = lrcvModel.bestModel.summary\n",
    "#print(\"Coefficient Standard Errors: \" + str(lrcvSummary.coefficientStandardErrors))\n",
    "#print(\"P Values: \" + str(lrcvSummary.pValues)) # Last element is the intercept\n",
    "\n",
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "lrpredictions = lrcvModel.transform(df_test)\n",
    "lrpredictions_train = lrcvModel.transform(df_train)\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "print('RMSE:', lrevaluator.evaluate(lrpredictions))\n",
    "                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|  Id|        prediction|\n",
      "+----+------------------+\n",
      "| 211| 97650.06711397617|\n",
      "| 319| 352638.2941583546|\n",
      "| 240|115739.37570982237|\n",
      "| 987|118567.21734835854|\n",
      "|1417|161679.35340367886|\n",
      "+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrpredictions_train.select(\"Id\",\"prediction\").show(5);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
